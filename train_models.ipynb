{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "me5HrTfminJ0",
        "outputId": "8a1b3694-a29d-41a3-f916-466fb94f84fb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting Sentencepiece\n",
            "  Downloading sentencepiece-0.1.96-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2 MB 4.3 MB/s \n",
            "\u001b[?25hInstalling collected packages: Sentencepiece\n",
            "Successfully installed Sentencepiece-0.1.96\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.15.0-py3-none-any.whl (3.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.4 MB 4.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.4.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Collecting tokenizers<0.11,>=0.10.1\n",
            "  Downloading tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3 MB 65.6 MB/s \n",
            "\u001b[?25hCollecting pyyaml>=5.1\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 77.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.8.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.0.46-py3-none-any.whl (895 kB)\n",
            "\u001b[K     |████████████████████████████████| 895 kB 75.4 MB/s \n",
            "\u001b[?25hCollecting huggingface-hub<1.0,>=0.1.0\n",
            "  Downloading huggingface_hub-0.2.1-py3-none-any.whl (61 kB)\n",
            "\u001b[K     |████████████████████████████████| 61 kB 471 kB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.6)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.6.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n",
            "Installing collected packages: pyyaml, tokenizers, sacremoses, huggingface-hub, transformers\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed huggingface-hub-0.2.1 pyyaml-6.0 sacremoses-0.0.46 tokenizers-0.10.3 transformers-4.15.0\n"
          ]
        }
      ],
      "source": [
        "!pip install Sentencepiece\n",
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "rN38mJAQzu6i"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from transformers import BertTokenizer, BigBirdTokenizer\n",
        "from transformers import BertForSequenceClassification, AdamW, BertConfig, BigBirdForSequenceClassification, GPT2Tokenizer, GPT2ForSequenceClassification\n",
        "from transformers import get_linear_schedule_with_warmup\n",
        "from torch.utils.data import TensorDataset, random_split\n",
        "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
        "import nltk\n",
        "\n",
        "from models import *"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zk2_6Vg30-PC",
        "outputId": "6c75d5f3-65ae-475e-af1b-56625b6ae32c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# from google.colab import drive\n",
        "# drive._mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "jrZnQzOl2u9b"
      },
      "outputs": [],
      "source": [
        "# import os\n",
        "# os.chdir('drive/MyDrive/machine_learning')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QcrN3SYzizGF",
        "outputId": "f4f2da6f-2e44-47dc-e56f-7f4c44c98a84"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train dataset length: 128\n",
            "Test dataset length: 128\n"
          ]
        }
      ],
      "source": [
        "# # load dataset\n",
        "\n",
        "data_train = pd.read_csv('dataset_binary_train.csv')[0:128]\n",
        "data_test = pd.read_csv('dataset_binary_test.csv')[0:128]\n",
        "\n",
        "X_train, y_train = data_train.data.tolist(), data_train.label.tolist()\n",
        "X_test, y_test = data_test.data.tolist(), data_test.label.tolist()\n",
        "\n",
        "print('Train dataset length: {}'.format(len(X_train)))\n",
        "print('Test dataset length: {}'.format(len(X_test)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "61dZO_rsizPs"
      },
      "outputs": [],
      "source": [
        "def train_val(train_dataset, test_dataset, transformer_name, transformer, classifier_name=None, classifier=None, lr_transformer=3e-5, lr_classifier=1e-3, batch_size=64, max_epoch=5):\n",
        "    # create dataloader for tensor dataset\n",
        "    train_dataloader = DataLoader(train_dataset, sampler = RandomSampler(train_dataset), batch_size = batch_size)\n",
        "    val_dataloader = DataLoader(test_dataset, sampler = SequentialSampler(test_dataset), batch_size = batch_size)\n",
        "    \n",
        "    # define device\n",
        "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "    \n",
        "    # use cuda for transformer\n",
        "    transformer = transformer.model.to(device)\n",
        "    \n",
        "    # define models\n",
        "    if classifier is not None:\n",
        "        classifier = classifier.to(device)\n",
        "        optimizer = torch.optim.Adam([{\"params\": classifier.parameters(), 'lr': lr_classifier}])\n",
        "        for p in transformer.parameters(): # freeze the layers of transformer\n",
        "            p.requires_grad = False\n",
        "    else:\n",
        "        optimizer = torch.optim.Adam(transformer.parameters(), lr = lr_transformer) # the learning rate is suggested by the authors\n",
        "\n",
        "        for p in transformer.parameters():\n",
        "            p.requires_grad = True\n",
        "\n",
        "    \n",
        "    # Hyper-parameters\n",
        "    max_epoch = 5\n",
        "    n_batch = int(len(train_dataset)/batch_size)\n",
        "    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps = 0, num_training_steps = len(train_dataloader) * max_epoch)\n",
        "    criterion = F.cross_entropy\n",
        "    \n",
        "    # clean memory in GPU\n",
        "    torch.cuda.empty_cache()\n",
        "    \n",
        "    # a list to record the state of training\n",
        "    training_stats = []\n",
        "\n",
        "\n",
        "            \n",
        "    print('Training start!')\n",
        "    for e in range(max_epoch):\n",
        "        \n",
        "        # train model\n",
        "        #model.train()\n",
        "        if classifier is not None:\n",
        "            classifier.train()\n",
        "        else:\n",
        "            transformer.train()\n",
        "        \n",
        "        epoch_loss = 0\n",
        "        train_acc = 0\n",
        "        \n",
        "        for b, (x_id, x_mask, y) in enumerate(train_dataloader):\n",
        "            x_id, x_mask, y = x_id.to(device), x_mask.to(device), y.to(device)\n",
        "            \n",
        "            optimizer.zero_grad()\n",
        "            \n",
        "            if classifier is not None:\n",
        "                with torch.no_grad():\n",
        "                    word_embedding = transformer(x_id, token_type_ids=None, attention_mask=x_mask, labels=y)['hidden_states'][-1]   \n",
        "                logits = classifier(word_embedding)\n",
        "                loss = criterion(logits, y)\n",
        "            else:\n",
        "                outputs = transformer(x_id, token_type_ids=None, attention_mask=x_mask, labels=y)\n",
        "                loss, logits = outputs['loss'], outputs['logits']\n",
        "\n",
        "\n",
        "            epoch_loss += loss\n",
        "            train_acc += (logits.max(1)[1] == y).float().mean().item()\n",
        "            \n",
        "            loss.backward()\n",
        "            \n",
        "            #clip gradient\n",
        "            if classifier is None:\n",
        "                torch.nn.utils.clip_grad_norm_(transformer.parameters(), 1.0)\n",
        "\n",
        "            optimizer.step()\n",
        "            scheduler.step()\n",
        "\n",
        "            print(\"\\rEpoch: {:d} batch: {:d} / {} loss: {:.4f} | {:.2%}\".format(e + 1, b, n_batch, loss, b*1.0/n_batch), end='', flush=True)\n",
        "        print(\"\\n----- Epoch {} ------\\nTraining loss: {}\".format(e+1, epoch_loss / len(train_dataloader)))\n",
        "        print(\"Training accuracy: {}\".format(train_acc / len(train_dataloader)))\n",
        "\n",
        "        \n",
        "        # evaluate model\n",
        "        if classifier is not None:\n",
        "            classifier.eval()\n",
        "        transformer.eval()\n",
        "        \n",
        "        eval_acc = 0\n",
        "        eval_loss = 0\n",
        "        nb_eval_steps = 0\n",
        "        \n",
        "        for b, (x_id, x_mask, y) in enumerate(val_dataloader):\n",
        "            x_id, x_mask, y = x_id.to(device), x_mask.to(device), y.to(device)\n",
        "            \n",
        "            with torch.no_grad():\n",
        "                if classifier is not None:\n",
        "                    word_embedding = transformer(x_id, token_type_ids=None, attention_mask=x_mask, labels=y)['hidden_states'][-1]   \n",
        "                    logits = classifier(word_embedding)\n",
        "                    loss = criterion(logits, y)\n",
        "                else:\n",
        "                    outputs = transformer(x_id, token_type_ids=None, attention_mask=x_mask, labels=y)\n",
        "                    loss, logits = outputs['loss'], outputs['logits']\n",
        "            \n",
        "            eval_loss += loss\n",
        "            eval_acc += (logits.max(1)[1] == y).float().mean().item()\n",
        "\n",
        "        print(\"Validation loss: {}\".format(eval_loss / len(val_dataloader)))\n",
        "        print(\"Validation accuracy: {}\".format(eval_acc / len(val_dataloader)))\n",
        "        print(\"\\n\")\n",
        "        \n",
        "        training_stats.append(\n",
        "            {\n",
        "                'epoch': e+1,\n",
        "                'train_loss': epoch_loss / len(train_dataloader),\n",
        "                'train_acc': train_acc / len(train_dataloader),\n",
        "                'val_loss': eval_loss / len(val_dataloader),\n",
        "                'val_acc': eval_acc / len(val_dataloader),\n",
        "            }\n",
        "        )\n",
        "\n",
        "        # save models\n",
        "        if classifier is not None:\n",
        "            torch.save(classifier, '{}-{}.pkl'.format(transformer_name, classifier_name))\n",
        "        else:\n",
        "            torch.save(transformer, '{}.pkl'.format(transformer_name))\n",
        "        \n",
        "        # save states of training\n",
        "        np.save('{}-{}-train_stats_Epoch{}.npy'.format(transformer_name, classifier_name, e+1), training_stats) \n",
        "\n",
        "    print('Training complete!')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ur4YhIiz1B0i"
      },
      "source": [
        "**Fine-tune BERT**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "PApSKotb1For",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "07c70684-e04a-4666-9787-30b7b8131be6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----- Processing 128/128 test samples\n",
            "\n",
            "Training start!\n",
            "Epoch: 1 batch: 1 / 2 loss: 0.6671 | 50.00%\n",
            "----- Epoch 1 ------\n",
            "Training loss: 0.6954854726791382\n",
            "Training accuracy: 0.5078125\n",
            "Validation loss: 0.7005167603492737\n",
            "Validation accuracy: 0.4921875\n",
            "\n",
            "\n",
            "Epoch: 2 batch: 1 / 2 loss: 0.6293 | 50.00%\n",
            "----- Epoch 2 ------\n",
            "Training loss: 0.6589019298553467\n",
            "Training accuracy: 0.6015625\n",
            "Validation loss: 0.6863597631454468\n",
            "Validation accuracy: 0.53125\n",
            "\n",
            "\n",
            "Epoch: 3 batch: 1 / 2 loss: 0.6014 | 50.00%\n",
            "----- Epoch 3 ------\n",
            "Training loss: 0.6126751899719238\n",
            "Training accuracy: 0.7421875\n",
            "Validation loss: 0.6732439398765564\n",
            "Validation accuracy: 0.59375\n",
            "\n",
            "\n",
            "Epoch: 4 batch: 1 / 2 loss: 0.5674 | 50.00%\n",
            "----- Epoch 4 ------\n",
            "Training loss: 0.5802326202392578\n",
            "Training accuracy: 0.78125\n",
            "Validation loss: 0.6695328950881958\n",
            "Validation accuracy: 0.6015625\n",
            "\n",
            "\n",
            "Epoch: 5 batch: 1 / 2 loss: 0.5474 | 50.00%\n",
            "----- Epoch 5 ------\n",
            "Training loss: 0.5640774965286255\n",
            "Training accuracy: 0.84375\n",
            "Validation loss: 0.6684213876724243\n",
            "Validation accuracy: 0.59375\n",
            "\n",
            "\n",
            "Training complete!\n"
          ]
        }
      ],
      "source": [
        "transformer = Transformer('BERT')\n",
        "\n",
        "train_dataset, test_dataset = transformer.preprocess_data(X_train, X_test, y_train, y_test)\n",
        "train_val(train_dataset, test_dataset, transformer_name='BERT', transformer=transformer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KiqhDJZs1GD4"
      },
      "source": [
        "**Fine-tune GPT2**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "Xp3pA1_G1NlM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aec7473e-a6cf-43c2-b874-4db4ebc0ba86"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at gpt2 and are newly initialized: ['score.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----- Processing 128/128 test samples\n",
            "\n",
            "Training start!\n",
            "Epoch: 1 batch: 1 / 2 loss: 1.8211 | 50.00%\n",
            "----- Epoch 1 ------\n",
            "Training loss: 2.349409341812134\n",
            "Training accuracy: 0.4140625\n",
            "Validation loss: 1.583845853805542\n",
            "Validation accuracy: 0.5390625\n",
            "\n",
            "\n",
            "Epoch: 2 batch: 1 / 2 loss: 1.5382 | 50.00%\n",
            "----- Epoch 2 ------\n",
            "Training loss: 1.7181981801986694\n",
            "Training accuracy: 0.4140625\n",
            "Validation loss: 1.2930728197097778\n",
            "Validation accuracy: 0.546875\n",
            "\n",
            "\n",
            "Epoch: 3 batch: 1 / 2 loss: 1.0474 | 50.00%\n",
            "----- Epoch 3 ------\n",
            "Training loss: 1.2786409854888916\n",
            "Training accuracy: 0.390625\n",
            "Validation loss: 1.1193398237228394\n",
            "Validation accuracy: 0.546875\n",
            "\n",
            "\n",
            "Epoch: 4 batch: 1 / 2 loss: 1.0663 | 50.00%\n",
            "----- Epoch 4 ------\n",
            "Training loss: 1.095575213432312\n",
            "Training accuracy: 0.453125\n",
            "Validation loss: 1.035834550857544\n",
            "Validation accuracy: 0.5546875\n",
            "\n",
            "\n",
            "Epoch: 5 batch: 1 / 2 loss: 0.8082 | 50.00%\n",
            "----- Epoch 5 ------\n",
            "Training loss: 0.9123766422271729\n",
            "Training accuracy: 0.4921875\n",
            "Validation loss: 1.0074186325073242\n",
            "Validation accuracy: 0.5625\n",
            "\n",
            "\n",
            "Training complete!\n"
          ]
        }
      ],
      "source": [
        "transformer = Transformer('GPT2')\n",
        "\n",
        "train_dataset, test_dataset = transformer.preprocess_data(X_train, X_test, y_train, y_test)\n",
        "train_val(train_dataset, test_dataset, transformer_name='GPT2', transformer=transformer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oalx5Ww41N7S"
      },
      "source": [
        "**Fine-tune BIGBIRD**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "vjKJpHod1QmD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c5f0e5fb-f1bf-46aa-e97d-7e162e5a2c5a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at google/bigbird-roberta-base were not used when initializing BigBirdForSequenceClassification: ['cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
            "- This IS expected if you are initializing BigBirdForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BigBirdForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BigBirdForSequenceClassification were not initialized from the model checkpoint at google/bigbird-roberta-base and are newly initialized: ['classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.out_proj.weight', 'classifier.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----- Processing 128/128 test samples\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Attention type 'block_sparse' is not possible if sequence_length: 100 <= num global tokens: 2 * config.block_size + min. num sliding tokens: 3 * config.block_size + config.num_random_blocks * config.block_size + additional buffer: config.num_random_blocks * config.block_size = 704 with config.block_size = 64, config.num_random_blocks = 3. Changing attention type to 'original_full'...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training start!\n",
            "Epoch: 1 batch: 1 / 2 loss: 0.6867 | 50.00%\n",
            "----- Epoch 1 ------\n",
            "Training loss: 0.6903771162033081\n",
            "Training accuracy: 0.546875\n",
            "Validation loss: 0.7005292177200317\n",
            "Validation accuracy: 0.4375\n",
            "\n",
            "\n",
            "Epoch: 2 batch: 1 / 2 loss: 0.6880 | 50.00%\n",
            "----- Epoch 2 ------\n",
            "Training loss: 0.6862771511077881\n",
            "Training accuracy: 0.5703125\n",
            "Validation loss: 0.7018076181411743\n",
            "Validation accuracy: 0.4609375\n",
            "\n",
            "\n",
            "Epoch: 3 batch: 1 / 2 loss: 0.6864 | 50.00%\n",
            "----- Epoch 3 ------\n",
            "Training loss: 0.6820776462554932\n",
            "Training accuracy: 0.5859375\n",
            "Validation loss: 0.7052717804908752\n",
            "Validation accuracy: 0.4609375\n",
            "\n",
            "\n",
            "Epoch: 4 batch: 1 / 2 loss: 0.6924 | 50.00%\n",
            "----- Epoch 4 ------\n",
            "Training loss: 0.6825997233390808\n",
            "Training accuracy: 0.5859375\n",
            "Validation loss: 0.7088981866836548\n",
            "Validation accuracy: 0.4609375\n",
            "\n",
            "\n",
            "Epoch: 5 batch: 1 / 2 loss: 0.6889 | 50.00%\n",
            "----- Epoch 5 ------\n",
            "Training loss: 0.6784468293190002\n",
            "Training accuracy: 0.5859375\n",
            "Validation loss: 0.71048504114151\n",
            "Validation accuracy: 0.4609375\n",
            "\n",
            "\n",
            "Training complete!\n"
          ]
        }
      ],
      "source": [
        "transformer = Transformer('BIGBIRD')\n",
        "\n",
        "train_dataset, test_dataset = transformer.preprocess_data(X_train, X_test, y_train, y_test)\n",
        "train_val(train_dataset, test_dataset, transformer_name='BIGBIRD', transformer=transformer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8PQKmQuT1Q7t"
      },
      "source": [
        "**BERT + BiLSTM**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "p-gMHbyR1YSe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "805e59c5-3f27-4b26-dbbc-0142ec1df13e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----- Processing 128/128 test samples\n",
            "\n",
            "Training start!\n",
            "Epoch: 1 batch: 1 / 2 loss: 0.7493 | 50.00%\n",
            "----- Epoch 1 ------\n",
            "Training loss: 0.7136397361755371\n",
            "Training accuracy: 0.578125\n",
            "Validation loss: 0.718366265296936\n",
            "Validation accuracy: 0.4609375\n",
            "\n",
            "\n",
            "Epoch: 2 batch: 1 / 2 loss: 0.6011 | 50.00%\n",
            "----- Epoch 2 ------\n",
            "Training loss: 0.6217240691184998\n",
            "Training accuracy: 0.65625\n",
            "Validation loss: 0.650848388671875\n",
            "Validation accuracy: 0.7109375\n",
            "\n",
            "\n",
            "Epoch: 3 batch: 1 / 2 loss: 0.5636 | 50.00%\n",
            "----- Epoch 3 ------\n",
            "Training loss: 0.5741910934448242\n",
            "Training accuracy: 0.859375\n",
            "Validation loss: 0.6382578611373901\n",
            "Validation accuracy: 0.71875\n",
            "\n",
            "\n",
            "Epoch: 4 batch: 1 / 2 loss: 0.5374 | 50.00%\n",
            "----- Epoch 4 ------\n",
            "Training loss: 0.5199998021125793\n",
            "Training accuracy: 0.890625\n",
            "Validation loss: 0.6255455017089844\n",
            "Validation accuracy: 0.6796875\n",
            "\n",
            "\n",
            "Epoch: 5 batch: 1 / 2 loss: 0.4614 | 50.00%\n",
            "----- Epoch 5 ------\n",
            "Training loss: 0.47346341609954834\n",
            "Training accuracy: 0.8984375\n",
            "Validation loss: 0.6181678771972656\n",
            "Validation accuracy: 0.6796875\n",
            "\n",
            "\n",
            "Training complete!\n"
          ]
        }
      ],
      "source": [
        "lstm = LSTM_attention()\n",
        "transformer = Transformer('BERT')\n",
        "\n",
        "train_dataset, test_dataset = transformer.preprocess_data(X_train, X_test, y_train, y_test)\n",
        "train_val(train_dataset, test_dataset, transformer_name='BERT', transformer=transformer, classifier_name='BiLSTM', classifier=lstm)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P4PfEBBs1Yex"
      },
      "source": [
        "**BERT + TextCNN**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "XKmzAhzh2hoe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "463af685-46c5-40be-f8a9-81bf509ffff6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----- Processing 128/128 test samples\n",
            "\n",
            "Training start!\n",
            "Epoch: 1 batch: 1 / 2 loss: 0.6952 | 50.00%\n",
            "----- Epoch 1 ------\n",
            "Training loss: 0.7091507911682129\n",
            "Training accuracy: 0.5078125\n",
            "Validation loss: 0.739465594291687\n",
            "Validation accuracy: 0.4609375\n",
            "\n",
            "\n",
            "Epoch: 2 batch: 1 / 2 loss: 0.7029 | 50.00%\n",
            "----- Epoch 2 ------\n",
            "Training loss: 0.6657478213310242\n",
            "Training accuracy: 0.5859375\n",
            "Validation loss: 0.7763028740882874\n",
            "Validation accuracy: 0.4609375\n",
            "\n",
            "\n",
            "Epoch: 3 batch: 1 / 2 loss: 0.6428 | 50.00%\n",
            "----- Epoch 3 ------\n",
            "Training loss: 0.6248608231544495\n",
            "Training accuracy: 0.5859375\n",
            "Validation loss: 0.739249587059021\n",
            "Validation accuracy: 0.4609375\n",
            "\n",
            "\n",
            "Epoch: 4 batch: 1 / 2 loss: 0.5904 | 50.00%\n",
            "----- Epoch 4 ------\n",
            "Training loss: 0.5903469920158386\n",
            "Training accuracy: 0.609375\n",
            "Validation loss: 0.7120996713638306\n",
            "Validation accuracy: 0.484375\n",
            "\n",
            "\n",
            "Epoch: 5 batch: 1 / 2 loss: 0.6077 | 50.00%\n",
            "----- Epoch 5 ------\n",
            "Training loss: 0.5723909139633179\n",
            "Training accuracy: 0.6875\n",
            "Validation loss: 0.7042070031166077\n",
            "Validation accuracy: 0.4921875\n",
            "\n",
            "\n",
            "Training complete!\n"
          ]
        }
      ],
      "source": [
        "textcnn = textCNN()\n",
        "transformer = Transformer('BERT')\n",
        "\n",
        "train_dataset, test_dataset = transformer.preprocess_data(X_train, X_test, y_train, y_test)\n",
        "train_val(train_dataset, test_dataset, transformer_name='BERT', transformer=transformer, classifier_name='TextCNN', classifier=textcnn)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Fine-tune BERT with large dataset**"
      ],
      "metadata": {
        "id": "4LBnlb3LVAoo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # load large dataset (~5 million sentences)\n",
        "\n",
        "# data_large_train = pd.read_csv('dataset_binary_train_large.csv')[0:128]\n",
        "# data_large_test = pd.read_csv('dataset_binary_test_large.csv')[0:128]\n",
        "\n",
        "data_large_train = pd.read_csv('dataset_binary_train.csv')[0:128]\n",
        "data_large_test = pd.read_csv('dataset_binary_test.csv')[0:128]\n",
        "\n",
        "X_large_train, y_large_train = data_large_train.data.tolist(), data_large_train.label.tolist()\n",
        "X_large_test, y_large_test = data_large_test.data.tolist(), data_large_test.label.tolist()\n",
        "\n",
        "print('Train dataset length: {}'.format(len(X_large_train)))\n",
        "print('Test dataset length: {}'.format(len(X_large_test)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T66JemkOU72B",
        "outputId": "43949d48-e0f6-4b2d-ae92-62f94b731170"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train dataset length: 128\n",
            "Test dataset length: 128\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "transformer = Transformer('BERT')\n",
        "\n",
        "train_dataset, test_dataset = transformer.preprocess_data(X_train, X_test, y_train, y_test)\n",
        "train_val(train_dataset, test_dataset, transformer_name='BERT', transformer=transformer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-0-toOtTVGGW",
        "outputId": "ccb8a02d-8695-40b2-9d0d-fb79fba80629"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----- Processing 128/128 test samples\n",
            "\n",
            "Training start!\n",
            "Epoch: 1 batch: 1 / 2 loss: 0.7575 | 50.00%\n",
            "----- Epoch 1 ------\n",
            "Training loss: 0.8150010108947754\n",
            "Training accuracy: 0.4296875\n",
            "Validation loss: 0.6841989159584045\n",
            "Validation accuracy: 0.5390625\n",
            "\n",
            "\n",
            "Epoch: 2 batch: 1 / 2 loss: 0.5919 | 50.00%\n",
            "----- Epoch 2 ------\n",
            "Training loss: 0.6073437929153442\n",
            "Training accuracy: 0.734375\n",
            "Validation loss: 0.7567552328109741\n",
            "Validation accuracy: 0.46875\n",
            "\n",
            "\n",
            "Epoch: 3 batch: 1 / 2 loss: 0.5796 | 50.00%\n",
            "----- Epoch 3 ------\n",
            "Training loss: 0.6150369644165039\n",
            "Training accuracy: 0.6328125\n",
            "Validation loss: 0.7781224250793457\n",
            "Validation accuracy: 0.46875\n",
            "\n",
            "\n",
            "Epoch: 4 batch: 1 / 2 loss: 0.5377 | 50.00%\n",
            "----- Epoch 4 ------\n",
            "Training loss: 0.5784770846366882\n",
            "Training accuracy: 0.609375\n",
            "Validation loss: 0.738916277885437\n",
            "Validation accuracy: 0.5\n",
            "\n",
            "\n",
            "Epoch: 5 batch: 1 / 2 loss: 0.5396 | 50.00%\n",
            "----- Epoch 5 ------\n",
            "Training loss: 0.534643292427063\n",
            "Training accuracy: 0.6796875\n",
            "Validation loss: 0.7135176062583923\n",
            "Validation accuracy: 0.515625\n",
            "\n",
            "\n",
            "Training complete!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Fine-tune BERT with multi-label data**"
      ],
      "metadata": {
        "id": "tB7X7k9gVIgm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "EHiOSW4GtgMQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4a325c79-656b-4572-c469-b16f18a7eb73"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train dataset length: 128\n",
            "Test dataset length: 128\n"
          ]
        }
      ],
      "source": [
        "# # load multi-label dataset\n",
        "\n",
        "data_multi_train = pd.read_csv('dataset_multi_num_train.csv')[0:128]\n",
        "data_multi_test = pd.read_csv('dataset_multi_num_test.csv')[0:128]\n",
        "\n",
        "X_multi_train, y_multi_train = data_multi_train.data.tolist(), data_multi_train.label.tolist()\n",
        "X_multi_test, y_multi_test = data_multi_test.data.tolist(), data_multi_test.label.tolist()\n",
        "\n",
        "print('Train dataset length: {}'.format(len(X_multi_train)))\n",
        "print('Test dataset length: {}'.format(len(X_multi_test)))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "transformer = Transformer('BERT', num_labels=5)\n",
        "\n",
        "train_dataset, test_dataset = transformer.preprocess_data(X_train, X_test, y_train, y_test)\n",
        "train_val(train_dataset, test_dataset, transformer_name='BERT', transformer=transformer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ym_KJB-kVPfl",
        "outputId": "47f441a8-2769-4934-8b46-f518b08b98e7"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----- Processing 128/128 test samples\n",
            "\n",
            "Training start!\n",
            "Epoch: 1 batch: 1 / 2 loss: 1.4157 | 50.00%\n",
            "----- Epoch 1 ------\n",
            "Training loss: 1.4839255809783936\n",
            "Training accuracy: 0.265625\n",
            "Validation loss: 1.3349766731262207\n",
            "Validation accuracy: 0.4296875\n",
            "\n",
            "\n",
            "Epoch: 2 batch: 1 / 2 loss: 1.2211 | 50.00%\n",
            "----- Epoch 2 ------\n",
            "Training loss: 1.2721401453018188\n",
            "Training accuracy: 0.5390625\n",
            "Validation loss: 1.1931195259094238\n",
            "Validation accuracy: 0.4609375\n",
            "\n",
            "\n",
            "Epoch: 3 batch: 1 / 2 loss: 1.0939 | 50.00%\n",
            "----- Epoch 3 ------\n",
            "Training loss: 1.1145927906036377\n",
            "Training accuracy: 0.6015625\n",
            "Validation loss: 1.0886192321777344\n",
            "Validation accuracy: 0.4609375\n",
            "\n",
            "\n",
            "Epoch: 4 batch: 1 / 2 loss: 1.0018 | 50.00%\n",
            "----- Epoch 4 ------\n",
            "Training loss: 1.0111143589019775\n",
            "Training accuracy: 0.59375\n",
            "Validation loss: 1.0257381200790405\n",
            "Validation accuracy: 0.4609375\n",
            "\n",
            "\n",
            "Epoch: 5 batch: 1 / 2 loss: 0.9933 | 50.00%\n",
            "----- Epoch 5 ------\n",
            "Training loss: 0.979364812374115\n",
            "Training accuracy: 0.6015625\n",
            "Validation loss: 1.0024933815002441\n",
            "Validation accuracy: 0.4609375\n",
            "\n",
            "\n",
            "Training complete!\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "train_models.ipynb",
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}