{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4B3dMnaMFGtR"
   },
   "outputs": [],
   "source": [
    "!pip install Sentencepiece\n",
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ojR46yoyFMF6"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import BertTokenizer, BigBirdTokenizer\n",
    "from transformers import BertForSequenceClassification, AdamW, BertConfig, BigBirdForSequenceClassification, GPT2Tokenizer, GPT2ForSequenceClassification\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from torch.utils.data import TensorDataset, random_split\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "import nltk\n",
    "\n",
    "from models import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "khod0pFOFlfc"
   },
   "outputs": [],
   "source": [
    "# # load dataset\n",
    "data_train = pd.read_csv('dataset_binary_train.csv')\n",
    "data_test = pd.read_csv('dataset_binary_test.csv')\n",
    "\n",
    "X_train, y_train = data_train.data.tolist(), data_train.label.tolist()\n",
    "X_test, y_test = data_test.data.tolist(), data_test.label.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z8jyAEOBIGsW"
   },
   "outputs": [],
   "source": [
    "def test_acc(test_dataset, transformer_name, transformer, classifier_name=None, classifier=None, batch_size=64):\n",
    "    \"\"\"This function is used to test each trained model\"\"\"\n",
    "    # create dataloader for tensor dataset\n",
    "    val_dataloader = DataLoader(test_dataset, sampler = SequentialSampler(test_dataset), batch_size = batch_size)\n",
    "    \n",
    "    # define device\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    \n",
    "    # use cuda for transformer\n",
    "    transformer = transformer.to(device)\n",
    "\n",
    "    # evaluate model\n",
    "    if classifier is not None:\n",
    "        classifier.eval()\n",
    "    transformer.eval()\n",
    "    \n",
    "    eval_acc = 0\n",
    "    eval_loss = 0\n",
    "    nb_eval_steps = 0\n",
    "    criterion = F.cross_entropy\n",
    "    \n",
    "    for b, (x_id, x_mask, y) in enumerate(val_dataloader):\n",
    "        x_id, x_mask, y = x_id.to(device), x_mask.to(device), y.to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            if classifier is not None:\n",
    "                word_embedding = transformer(x_id, token_type_ids=None, attention_mask=x_mask, labels=y)['hidden_states'][-1]   \n",
    "                logits = classifier(word_embedding)\n",
    "                loss = criterion(logits, y)\n",
    "            else:\n",
    "                outputs = transformer(x_id, token_type_ids=None, attention_mask=x_mask, labels=y)\n",
    "                loss, logits = outputs['loss'], outputs['logits']\n",
    "        \n",
    "        eval_loss += loss\n",
    "        eval_acc += (logits.max(1)[1] == y).float().mean().item()\n",
    "\n",
    "    print(\"Validation loss: {}\".format(eval_loss / len(val_dataloader)))\n",
    "    print(\"Validation accuracy: {}\".format(eval_acc / len(val_dataloader)))\n",
    "    print(\"\\n\")\n",
    "\n",
    "    print('The test accuracy of {} {} is {}'.format(transformer_name, classifier_name, eval_acc / len(val_dataloader)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "efIc-8ipI-HT"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "41g6BkViI_mC"
   },
   "source": [
    "**Test fine-tuned BERT**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jTzrwqNaNRqx"
   },
   "outputs": [],
   "source": [
    "transformer = Transformer('BERT')\n",
    "test_dataset = transformer.preprocess_data(X_train=None, X_test=X_test, y_train=None, y_test=y_test)\n",
    "\n",
    "bert = torch.load('bert-unfreeze.pkl')\n",
    "test_acc(test_dataset, transformer_name='BERT', transformer=bert)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MKOkS9yUOskK"
   },
   "source": [
    "**Test fine-tuned GPT2**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jaTP9r8DI-XS"
   },
   "outputs": [],
   "source": [
    "transformer = Transformer('GPT2')\n",
    "#test_dataset = transformer.preprocess_data(X_train=None, X_test=X_test, y_train=None, y_test=y_test)\n",
    "\n",
    "gpt2 = torch.load('fc-gpt2.pkl')\n",
    "test_acc(test_dataset, transformer_name='GPT2', transformer=gpt2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RqPLKJTZPCck"
   },
   "source": [
    "**Test fine-tuned BIGBIRD**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m62Cr9lpI-Zp"
   },
   "outputs": [],
   "source": [
    "transformer = Transformer('BIGBIRD')\n",
    "test_dataset = transformer.preprocess_data(X_train=None, X_test=X_test, y_train=None, y_test=y_test)\n",
    "\n",
    "bigbird = torch.load('fc-bigbird-epoch3.pkl')\n",
    "test_acc(test_dataset, transformer_name='BIGBIRD', transformer=bigbird)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_X5RrTZHPWlD"
   },
   "source": [
    "**Test fine-tuned BERT + BiLSTM**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zRw3UG1sI-bs"
   },
   "outputs": [],
   "source": [
    "lstm = torch.load('LSTM-bert-embedding-LSTM.pkl')\n",
    "bert = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels = 2, output_attentions = False, output_hidden_states = True)\n",
    "transformer = Transformer('BERT')\n",
    "\n",
    "test_dataset = transformer.preprocess_data(X_train=None, X_test=X_test, y_train=None, y_test=y_test)\n",
    "test_acc(test_dataset, transformer_name='BERT', transformer=bert, classifier_name='BiLSTM', classifier=lstm)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Sl2AtCTpPvgR"
   },
   "source": [
    "**Test fine-tuned BERT + TextCNN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0BdIOfkII-di"
   },
   "outputs": [],
   "source": [
    "textcnn = torch.load('cnn-embedding.pkl')\n",
    "bert = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels = 2, output_attentions = False, output_hidden_states = True)\n",
    "transformer = Transformer('BERT')\n",
    "\n",
    "test_dataset = transformer.preprocess_data(X_train=None, X_test=X_test, y_train=None, y_test=y_test)\n",
    "test_acc(test_dataset, transformer_name='BERT', transformer=bert, classifier_name='TextCNN', classifier=textcnn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gunLQNo_QBpW"
   },
   "source": [
    "**Test fine-tuned BERT with large dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rKRzqHPeI-fc"
   },
   "outputs": [],
   "source": [
    "transformer = Transformer('BERT')\n",
    "bert = torch.load('bert-large.pkl')\n",
    "\n",
    "test_dataset = transformer.preprocess_data(X_train=None, X_test=X_test, y_train=None, y_test=y_test)\n",
    "test_acc(test_dataset, transformer_name='BERT', transformer=bert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pRFar9eMrA4Z"
   },
   "outputs": [],
   "source": [
    "# # load large dataset (~5 million sentences)\n",
    "\n",
    "data_large_train = pd.read_csv('dataset_binary_train_large.csv')\n",
    "data_large_test = pd.read_csv('dataset_binary_test_large.csv')\n",
    "\n",
    "\n",
    "X_large_train, y_large_train = data_large_train.data.tolist(), data_large_train.label.tolist()\n",
    "X_large_test, y_large_test = data_large_test.data.tolist(), data_large_test.label.tolist()\n",
    "\n",
    "print('Train dataset length: {}'.format(len(X_large_train)))\n",
    "print('Test dataset length: {}'.format(len(X_large_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wJm1Llm1rDMf"
   },
   "outputs": [],
   "source": [
    "transformer = Transformer('BERT')\n",
    "bert = torch.load('bert-large.pkl')\n",
    "\n",
    "test_dataset = transformer.preprocess_data(X_train=None, X_test=X_large_test, y_train=None, y_test=y_large_test)\n",
    "test_acc(test_dataset, transformer_name='BERT', transformer=bert)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o-h0Ld2NSpsy"
   },
   "source": [
    "**Test fine-tuned BERT with multi-label data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "avH4viSUI-jj"
   },
   "outputs": [],
   "source": [
    "# # load multi-label dataset\n",
    "\n",
    "data_multi_train = pd.read_csv('dataset_multi_num_train.csv')[0:128]\n",
    "data_multi_test = pd.read_csv('dataset_multi_num_test.csv')[0:128]\n",
    "\n",
    "X_multi_train, y_multi_train = data_multi_train.data.tolist(), data_multi_train.label.tolist()\n",
    "X_multi_test, y_multi_test = data_multi_test.data.tolist(), data_multi_test.label.tolist()\n",
    "\n",
    "print('Train dataset length: {}'.format(len(X_multi_train)))\n",
    "print('Test dataset length: {}'.format(len(X_multi_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "myZVq4iZq-Qb"
   },
   "outputs": [],
   "source": [
    "transformer = Transformer('BERT')\n",
    "bert = torch.load('bert-multi.pkl')\n",
    "\n",
    "test_dataset = transformer.preprocess_data(X_train=None, X_test=X_multi_test, y_train=None, y_test=y_multi_test)\n",
    "test_acc(test_dataset, transformer_name='BERT', transformer=bert)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "reproduce.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
